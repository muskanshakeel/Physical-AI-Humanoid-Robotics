"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9650],{4544:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module4/chapter6-casestudy","title":"Module 4: Vision-Language-Action (VLA) - Chapter 6: Capstone Project","description":"1. Concepts","source":"@site/docs/module4/chapter6-casestudy.md","sourceDirName":"module4","slug":"/module4/chapter6-casestudy","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter6-casestudy","draft":false,"unlisted":false,"editUrl":"https://github.com/muskanshakeel/Physical-AI-Humanoid-Robotics/tree/main/my-website/docs/module4/chapter6-casestudy.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA) - Chapter 5: Mini Project","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter5-miniproject"},"next":{"title":"Module 4: Vision-Language-Action (VLA) - Chapter 7: Debugging & Common Failures","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter7-debugging"}}');var o=t(4848),s=t(8453);const a={sidebar_position:6},r="Module 4: Vision-Language-Action (VLA) - Chapter 6: Capstone Project",c={},l=[{value:"1. Concepts",id:"1-concepts",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-vision-language-action-vla---chapter-6-capstone-project",children:"Module 4: Vision-Language-Action (VLA) - Chapter 6: Capstone Project"})}),"\n",(0,o.jsx)(n.h2,{id:"1-concepts",children:"1. Concepts"}),"\n",(0,o.jsx)(n.p,{children:"This Capstone Project brings together all the concepts and tools learned throughout the textbook to build an Autonomous Humanoid system capable of responding to complex voice commands. It integrates perception (vision), language understanding, high-level planning, navigation, object detection, and manipulation into a unified Vision-Language-Action pipeline."}),"\n",(0,o.jsx)("h2",{children:" 2. Tooling"}),"\n",(0,o.jsx)(n.p,{children:"This project will utilize a wide array of tools: Whisper for speech-to-text, an LLM API for planning and task decomposition, ROS 2 (rclpy, Actions, Nav2), Isaac Sim for high-fidelity simulation, and potentially Isaac ROS for perception tasks (VSLAM, object detection)."}),"\n",(0,o.jsx)("h2",{children:" 3. Implementation Walkthrough"}),"\n",(0,o.jsx)(n.p,{children:"This section will provide high-level guidance for the Capstone Project, outlining the integration points and system architecture without going into granular detail for each sub-component (which are covered in previous chapters)."}),"\n",(0,o.jsx)("h2",{children:" 4. Case study / example: Autonomous Humanoid"}),"\n",(0,o.jsx)(n.p,{children:"This section details the design and implementation of an autonomous humanoid robot that can understand voice commands to perform a complex sequence of tasks."}),"\n",(0,o.jsx)("h3",{children:" Project Objective"}),"\n",(0,o.jsx)(n.p,{children:'The goal is to enable a simulated (or real, if hardware is available) humanoid robot to respond to a voice command like "Robot, please find the red cup on the table and bring it to me." This command will trigger a sequence of actions:'}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Input"}),": Transcribe the command using Whisper."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"LLM Planning"}),": The LLM will decompose the command into sub-tasks (e.g., Navigate to table, Detect red cup, Manipulate to grasp, Navigate to user, Manipulate to place)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation"}),": Use Nav2 to plan and execute a path to the table."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Detection"}),": Employ vision systems (e.g., with Isaac ROS) to detect the red cup on the table."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation"}),": Use the robot's arm to grasp the red cup."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Final Navigation/Placement"}),": Navigate back to the user and place the cup."]}),"\n"]}),"\n",(0,o.jsx)("h3",{children:" System Architecture"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio Processing"}),": Microphone array -> Whisper -> Text"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"High-Level Planner"}),": Text -> LLM -> ROS 2 Action Graph"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception"}),": Camera -> Isaac ROS VSLAM / Object Detection"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation"}),": Nav2 stack (global/local planners, controllers)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation"}),": Inverse kinematics, grasp planner, ROS 2 Action server"]}),"\n"]}),"\n",(0,o.jsx)("h2",{children:" 5. Mini project"}),"\n",(0,o.jsx)(n.p,{children:"A hands-on project to create a robust fault-tolerance mechanism for one sub-component of the VLA pipeline (e.g., error recovery for failed grasping attempts)."}),"\n",(0,o.jsx)("h2",{children:" 6. Debugging & common failures"}),"\n",(0,o.jsx)(n.p,{children:"Debugging complex VLA pipelines involves challenges across all components: speech recognition errors, LLM planning failures, navigation stack issues, perception inaccuracies, and manipulation failures. This section will focus on system-level debugging strategies."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);
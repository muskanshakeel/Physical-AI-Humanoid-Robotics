"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8822],{3087:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>l,frontMatter:()=>r,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"module4/chapter3-implementation","title":"Module 4: Vision-Language-Action (VLA) - Chapter 3: Implementation Walkthrough","description":"1. Concepts","source":"@site/docs/module4/chapter3-implementation.md","sourceDirName":"module4","slug":"/module4/chapter3-implementation","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter3-implementation","draft":false,"unlisted":false,"editUrl":"https://github.com/muskanshakeel/Physical-AI-Humanoid-Robotics/tree/main/my-website/docs/module4/chapter3-implementation.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA) - Chapter 2: Tooling","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter2-tooling"},"next":{"title":"Module 4: Vision-Language-Action (VLA) - Chapter 4: Case Study / Example","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter4-casestudy"}}');var o=t(4848),s=t(8453);const r={sidebar_position:3},a="Module 4: Vision-Language-Action (VLA) - Chapter 3: Implementation Walkthrough",c={},h=[{value:"1. Concepts",id:"1-concepts",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-vision-language-action-vla---chapter-3-implementation-walkthrough",children:"Module 4: Vision-Language-Action (VLA) - Chapter 3: Implementation Walkthrough"})}),"\n",(0,o.jsx)(n.h2,{id:"1-concepts",children:"1. Concepts"}),"\n",(0,o.jsx)(n.p,{children:"This chapter provides a practical walkthrough of setting up and using OpenAI's Whisper model for speech-to-text transcription. Accurate transcription of spoken commands is the critical first step in a Vision-Language-Action (VLA) pipeline, enabling robots to understand human instructions."}),"\n",(0,o.jsx)("h2",{children:" 2. Tooling"}),"\n",(0,o.jsxs)(n.p,{children:["We will utilize Python, the ",(0,o.jsx)(n.code,{children:"openai-whisper"})," library, and a microphone for audio input. A command-line interface (CLI) will be used to run the transcription."]}),"\n",(0,o.jsx)("h2",{children:" 3. Implementation Walkthrough: Whisper for Speech-to-Text Processing"}),"\n",(0,o.jsx)(n.p,{children:"This section provides a detailed walkthrough to implement a basic speech-to-text system using Whisper."}),"\n",(0,o.jsx)("h3",{children:" Step 1: Install Whisper"}),"\n",(0,o.jsxs)(n.p,{children:["Instructions on how to install the ",(0,o.jsx)(n.code,{children:"openai-whisper"})," Python package."]}),"\n",(0,o.jsx)("h3",{children:" Step 2: Record Audio"}),"\n",(0,o.jsx)(n.p,{children:"Guidance on how to record a short audio clip (e.g., a spoken command for a robot) using system tools or Python libraries."}),"\n",(0,o.jsx)("h3",{children:" Step 3: Transcribe Audio with Whisper"}),"\n",(0,o.jsx)(n.p,{children:"Commands and Python script examples for loading a Whisper model and transcribing the recorded audio file."}),"\n",(0,o.jsx)("h3",{children:" Step 4: Process the Text Output"}),"\n",(0,o.jsx)(n.p,{children:"Basic Python code to process the transcribed text, such as normalizing it or extracting keywords, for further use in a robotics context."}),"\n",(0,o.jsx)("h2",{children:" 4. Case study / example"}),"\n",(0,o.jsxs)(n.p,{children:["A case study on integrating Whisper with a ROS 2 node that transcribes spoken commands and publishes them as ",(0,o.jsx)(n.code,{children:"std_msgs/String"})," messages."]}),"\n",(0,o.jsx)("h2",{children:" 5. Mini project"}),"\n",(0,o.jsx)(n.p,{children:"A hands-on project to evaluate Whisper's transcription accuracy in different noise environments or with various accents."}),"\n",(0,o.jsx)("h2",{children:" 6. Debugging & common failures"}),"\n",(0,o.jsx)(n.p,{children:"Common issues encountered during Whisper setup or usage, such as audio device conflicts, incorrect audio formats, model loading errors, or transcription inaccuracies."})]})}function l(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);
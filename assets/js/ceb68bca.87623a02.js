"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1801],{4469:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>c,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module4/chapter1-concepts","title":"Module 4: Vision-Language-Action (VLA) - Chapter 1: Concepts","description":"1. Concepts: Building Vision-Language-Action Pipelines for Humanoid Robotics","source":"@site/docs/module4/chapter1-concepts.md","sourceDirName":"module4","slug":"/module4/chapter1-concepts","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter1-concepts","draft":false,"unlisted":false,"editUrl":"https://github.com/muskanshakeel/Physical-AI-Humanoid-Robotics/tree/main/my-website/docs/module4/chapter1-concepts.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac) - Chapter 6: Debugging & Common Failures","permalink":"/Physical-AI-Humanoid-Robotics/docs/module3/chapter6-debugging"},"next":{"title":"Module 4: Vision-Language-Action (VLA) - Chapter 2: Tooling","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter2-tooling"}}');var t=i(4848),s=i(8453);const a={sidebar_position:1},c="Module 4: Vision-Language-Action (VLA) - Chapter 1: Concepts",r={},l=[{value:"1. Concepts: Building Vision-Language-Action Pipelines for Humanoid Robotics",id:"1-concepts-building-vision-language-action-pipelines-for-humanoid-robotics",level:2},{value:"2. Tooling",id:"2-tooling",level:2}];function d(n){const e={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"module-4-vision-language-action-vla---chapter-1-concepts",children:"Module 4: Vision-Language-Action (VLA) - Chapter 1: Concepts"})}),"\n",(0,t.jsx)(e.h2,{id:"1-concepts-building-vision-language-action-pipelines-for-humanoid-robotics",children:"1. Concepts: Building Vision-Language-Action Pipelines for Humanoid Robotics"}),"\n",(0,t.jsx)(e.p,{children:"This chapter introduces the Vision-Language-Action (VLA) paradigm, a cutting-edge approach that integrates perception, language understanding, and robotic action. We will explore how to create intelligent robots that can interpret human commands and execute complex tasks in the physical world. We will cover:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"VLA Pipeline Overview"}),": Understanding the end-to-end flow from sensory input (vision, audio) to high-level language understanding, task planning, and physical execution via robotic actions."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Whisper (Speech-to-Text)"}),": Introduction to OpenAI's Whisper model for accurate speech recognition, enabling robots to understand spoken commands."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"LLM Planning (Large Language Model-based Planning)"}),": Leveraging the reasoning and generation capabilities of Large Language Models (LLMs) to translate natural language commands into structured robot plans."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"ROS 2 Action Graph"}),": How LLM-generated plans can be converted into a sequence of ROS 2 actions for execution on a robotic platform."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Humanoid Robotics Context"}),": Specific considerations for VLA in humanoid robotics, including embodiment, balance, and complex manipulation."]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"2-tooling",children:"2. Tooling"}),"\n",(0,t.jsx)(e.p,{children:"This section will introduce the tools and frameworks required to build VLA pipelines, including Whisper, LLM APIs, and ROS 2."}),"\n",(0,t.jsx)("h2",{children:" 3. Implementation walkthrough"}),"\n",(0,t.jsx)(e.p,{children:"A step-by-step guide to setting up Whisper for transcribing audio commands."}),"\n",(0,t.jsx)("h2",{children:" 4. Case study / example"}),"\n",(0,t.jsx)(e.p,{children:"A case study on using an LLM to generate a sequence of high-level actions from a natural language instruction."}),"\n",(0,t.jsx)("h2",{children:" 5. Mini project"}),"\n",(0,t.jsx)(e.p,{children:"A hands-on project to convert an LLM-generated plan into executable ROS 2 action calls."}),"\n",(0,t.jsx)("h2",{children:" 6. Debugging & common failures"}),"\n",(0,t.jsx)(e.p,{children:"Common issues encountered in VLA pipelines, such as speech recognition errors, LLM hallucination in planning, or failures in action execution."})]})}function u(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>c});var o=i(6540);const t={},s=o.createContext(t);function a(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);
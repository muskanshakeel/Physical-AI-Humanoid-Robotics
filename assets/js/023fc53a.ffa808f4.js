"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[5838],{834:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module4/chapter2-tooling","title":"Module 4: Vision-Language-Action (VLA) - Chapter 2: Tooling","description":"1. Concepts","source":"@site/docs/module4/chapter2-tooling.md","sourceDirName":"module4","slug":"/module4/chapter2-tooling","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter2-tooling","draft":false,"unlisted":false,"editUrl":"https://github.com/muskanshakeel/Physical-AI-Humanoid-Robotics/tree/main/my-website/docs/module4/chapter2-tooling.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA) - Chapter 1: Concepts","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter1-concepts"},"next":{"title":"Module 4: Vision-Language-Action (VLA) - Chapter 3: Implementation Walkthrough","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter3-implementation"}}');var t=i(4848),s=i(8453);const r={sidebar_position:2},a="Module 4: Vision-Language-Action (VLA) - Chapter 2: Tooling",l={},c=[{value:"1. Concepts",id:"1-concepts",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla---chapter-2-tooling",children:"Module 4: Vision-Language-Action (VLA) - Chapter 2: Tooling"})}),"\n",(0,t.jsx)(n.h2,{id:"1-concepts",children:"1. Concepts"}),"\n",(0,t.jsx)(n.p,{children:"This chapter focuses on the practical tools and services required to assemble a Vision-Language-Action (VLA) pipeline. We will cover the setup of speech-to-text models and the integration of Large Language Models (LLMs) for high-level planning."}),"\n",(0,t.jsx)("h2",{children:" 2. Tooling: VLA Specific Setup"}),"\n",(0,t.jsx)(n.p,{children:"This section will detail the practical usage and setup of:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Whisper Setup"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Installation of OpenAI's Whisper model (e.g., via ",(0,t.jsx)(n.code,{children:"pip install openai-whisper"}),")."]}),"\n",(0,t.jsx)(n.li,{children:"Basic usage for transcribing audio from files or live streams."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLM API Integration"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Choosing an LLM provider (e.g., OpenAI, Anthropic, Gemini)."}),"\n",(0,t.jsx)(n.li,{children:"Setting up API keys and authentication."}),"\n",(0,t.jsx)(n.li,{children:"Making basic API calls to an LLM for text generation and instruction following."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Actions"}),": Review of ROS 2 Action servers and clients for executing planned robot behaviors."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Microphone Array"}),": Hardware considerations for robust audio input in robotics."]}),"\n"]}),"\n",(0,t.jsx)("h2",{children:" 3. Implementation Walkthrough"}),"\n",(0,t.jsx)(n.p,{children:"A step-by-step guide to installing Whisper and transcribing a sample audio file."}),"\n",(0,t.jsx)("h2",{children:" 4. Case study / example"}),"\n",(0,t.jsx)(n.p,{children:"A case study on using an LLM to rephrase a simple robot command into multiple variations to improve robustness."}),"\n",(0,t.jsx)("h2",{children:" 5. Mini project"}),"\n",(0,t.jsx)(n.p,{children:"A hands-on project to integrate a custom speech command into a ROS 2 system that triggers a simple robot movement."}),"\n",(0,t.jsx)("h2",{children:" 6. Debugging & common failures"}),"\n",(0,t.jsx)(n.p,{children:"Common issues encountered when setting up Whisper (e.g., audio device configuration, model loading errors) or integrating with LLM APIs (e.g., API key issues, rate limits, prompt engineering challenges)."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);
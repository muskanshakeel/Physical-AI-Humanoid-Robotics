"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9219],{4288:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module4/chapter5-miniproject","title":"Module 4: Vision-Language-Action (VLA) - Chapter 5: Mini Project","description":"1. Concepts","source":"@site/docs/module4/chapter5-miniproject.md","sourceDirName":"module4","slug":"/module4/chapter5-miniproject","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter5-miniproject","draft":false,"unlisted":false,"editUrl":"https://github.com/muskanshakeel/Physical-AI-Humanoid-Robotics/tree/main/my-website/docs/module4/chapter5-miniproject.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA) - Chapter 4: Case Study / Example","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter4-casestudy"},"next":{"title":"Module 4: Vision-Language-Action (VLA) - Chapter 6: Capstone Project","permalink":"/Physical-AI-Humanoid-Robotics/docs/module4/chapter6-casestudy"}}');var t=i(4848),s=i(8453);const r={sidebar_position:5},a="Module 4: Vision-Language-Action (VLA) - Chapter 5: Mini Project",c={},l=[{value:"1. Concepts",id:"1-concepts",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla---chapter-5-mini-project",children:"Module 4: Vision-Language-Action (VLA) - Chapter 5: Mini Project"})}),"\n",(0,t.jsx)(n.h2,{id:"1-concepts",children:"1. Concepts"}),"\n",(0,t.jsx)(n.p,{children:"This mini-project integrates speech recognition (Whisper), LLM-based planning, and ROS 2 action execution to create a simple end-to-end Vision-Language-Action pipeline. You will enable a robot to respond to basic voice commands."}),"\n",(0,t.jsx)("h2",{children:" 2. Tooling"}),"\n",(0,t.jsx)(n.p,{children:"We will utilize Whisper for speech-to-text, an LLM API for planning, and ROS 2 for defining and executing robot actions."}),"\n",(0,t.jsx)("h2",{children:" 3. Implementation Walkthrough"}),"\n",(0,t.jsx)(n.p,{children:"This section will provide high-level guidance for the mini-project, assuming you will fill in the details based on previous chapters."}),"\n",(0,t.jsx)("h2",{children:" 4. Case study / example"}),"\n",(0,t.jsx)(n.p,{children:"A case study on developing a simplified voice interface for a smart home assistant that controls lights and appliances."}),"\n",(0,t.jsx)("h2",{children:" 5. Mini Project: Simple Voice Command to Robot Action"}),"\n",(0,t.jsx)(n.p,{children:"The goal of this mini-project is to build a basic VLA system where a robot (simulated or real) can perform a simple action based on a spoken command."}),"\n",(0,t.jsx)("h3",{children:" Project Objective"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice Command Input"}),': Use Whisper to transcribe a spoken command (e.g., "robot, move forward").']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLM Interpretation"}),": Feed the transcribed text to an LLM to generate a corresponding robot action (e.g., ",(0,t.jsx)(n.code,{children:"move_forward(distance=1.0)"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Action Execution"}),": Convert the LLM's output into a ROS 2 action goal and send it to a robot action server (e.g., a simple robot controller that moves forward)."]}),"\n"]}),"\n",(0,t.jsx)("h3",{children:" Validation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The robot correctly interprets the voice command and executes the corresponding action."}),"\n"]}),"\n",(0,t.jsx)("h2",{children:" 6. Debugging & common failures"}),"\n",(0,t.jsx)(n.p,{children:"Common issues in this integrated pipeline, such as misinterpretations by Whisper, LLM planning errors, or failures in translating LLM output to valid ROS 2 actions."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);